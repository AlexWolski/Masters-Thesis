% !TeX root = Thesis.tex

\chapter{Introduction}
\label{chap:introduction}

Recent advances in science, technology, and engineering have created a demand for efficient 3D shape reconstruction and analysis. Applications include autonomous navigation, robotics, medicine, augmented reality, and entertainment~\cite{Xiao2020, Xie2022}. These fields require methods to quickly processes 3D scenes and accurately reconstruct surfaces from sparse sensor data.

Traditional methods for 3D shape analysis require features to be manually engineered using prior knowledge of a problem. Although engineered features are easy to interpret, the process is laborious and overlooks potentially useful information. Research efforts have since shifted to deep learning techniques that automatically learn the most relevant features~\cite{Bengio2013}. The improved feature extraction allows for more complex and varied reconstructions than is possible using traditional methods. In addition, the learned feature vectors can be applied to analysis problems such as object classification and inference of missing geometry~\cite{Park2019}.

One of the primary challenges in applying deep learning to 3D reconstruction is selecting a suitable representation for the inputs and outputs of a neural network. The representation must be concise enough for effective training while retaining enough information for an accurate reproduction. Related work employs a variety of 3D representations, each with unique benefits and challenges~\cite{Xiao2020}. These representations are further discussed in the \nameref{sec:3d_representations} section of the \nameref{chap:related_work} chapter.

Recent work~\cite{Sharma2018, Kania2020, Ren2021} has explored the use of Constructive Solid Geometry (CSG) in deep learning. A CSG model defines a volume as a composition of simple shape primitives. Primitives are combined using boolean operations such as union, difference, and intersection. Each primitive is transformed using translation, rotation, and scaling operations. The available primitive shapes are restricted to a predefined set of manifold surfaces. CSG models are more compact than than other 3D representations. And by virtue of their construction, all CSG models are continuous and manifold surfaces bounding an interior volume. Additionally, editing a CSG model is convenient since each constituent primitive can be independently modified~\cite{Hughes2013}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Union}
		\caption{Union}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Difference}
		\caption{Difference}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Intersect}
		\caption{Intersect}
	\end{subfigure}
	\caption{Three boolean operations applied to two spheres: (a) Union operation includes points encompassed by either sphere, (b) Difference operation includes points encompassed exclusively by the first sphere, (c) Intersect operation includes points encompassed by both spheres simultaneously.}
	\label{fig:boolean operations}
\end{figure}

CSG and related primitive fitting techniques are topics of interest as they resemble Biederman's Recognition-By-Components theory~\cite{Biederman1987} for how humans comprehend 3D shapes. The theory suggests that the human brain deconstructs scenes into simple volumes called geons. Every brain is said to recognize same set of 36 geon primitives when processing volumes~\cite{Biederman1987}. Due to their similarity, researchers have suggested that the Recognition-By-Components theory sets a positive precedent for the use of CSG in deep learning~\cite{Sharma2018}.

Prior work~\cite{Sharma2018, Kania2020, Ren2021} represents CSG primitives as Signed Distance Fields (SDFs). An SDF is a mathematical function that implicitly defines the surface of a volume. Given a query point, an SDF returns the shortest distance from the query point to the represented surface. This distance is positive ($+$) when the query point is outside of the volume and negative ($-$) when the query point is inside. We define the surface as the set of all input points for which the SDF returns 0 (i.e. the zero-level set). The implicit nature of SDFs results in smooth, detailed, and continuous surfaces~\cite{Park2019}. And the boolean operations required for CSG construction can be easily applied to SDFs.

\begin{figure}
	\centering
	\includegraphics[scale=0.2]{Images/SDF Box}
	\caption{Visualization of the SDF of a 2D box. The surface of the box is depicted in black, the exterior in green, and the interior in red. Points farther from the surface are colored darker. Each of the white isoline consist of points that are equidistant from the surface.}
	\label{fig:sdf_box}
\end{figure}

There are several benefits to using CSG models for deep 3D reconstruction. The low-dimensionality of a CSG model allows for more efficient training of neural network. The continuous and watertight surfaces produced by CSG models are visually pleasing. And a key benefit missing from most representations is an easily modifiable format.

However, there are also several challenges in using CSG models. The first difficulty is that the implicit SDF shapes need to be discretized before being fed through a neural network or computing accuracy metrics. The extra step adds complexity and computational overhead. The second difficulty is that the lack of uniqueness in CSG models make them unsuited for supervised learning. With CSG modeling, an L-shaped prism can be constructed by either taking the union of two small cuboids or subtracting a small cuboid from a large cuboid~\cite{Hughes2013}. While both constructions are equally optimal, only one can be used as the ground-truth label. Selecting only one of many possible constructions as the ground-truth restricts the solution space and inhibits training. Lastly, the boolean operations used in CSG modeling have a tendency to create sharp seams at the intersection between primitives. There are methods for blending primitives to reduce seams, but the added feature comes at the cost of network complexity.

Prior work~\cite{Sharma2018, Kania2020, Ren2021} successfully implements CSG based reconstruction algorithms using unsupervised learning. While succeeding in representing simple geometry, these works fail to incorporate primitive blending to address the poor reconstruction quality of smooth surfaces. Furthermore, these models generate a small number of primitives. The CSG-Stump architecture~\cite{Ren2021} supports a maximum output of 256 total primitives in the paper, of which a subset is selected for use in reconstruction. This number is sufficient to represent simple 3D shapes but fails to capture the details in more complex geometry. Although the architecture can be scaled up, the network complexity, memory requirements, and training time increase non-linearly with the output size~\cite{Ren2021}.

\begin{figure}[!b]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Face without blending}
		\caption{Without Blending}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Face with blending}
		\caption{With Blending}
	\end{subfigure}
	\caption{Demonstration of the importance of primitive blending for modeling organic shapes such as faces using CSG~\cite{Quilez2013}.}
	\label{fig:primitive_blending}
\end{figure}

To address these shortcomings, we propose the Constructive Solid Geometry Cascaded Refinement Network (CSG-CRN): a novel architecture for iteratively generating CSG reconstructions of unlimited size. This is achieved using cascaded refinement, wherein a refinement network recursively improves upon its own output to achieve high reconstruction quality with a small architecture. CSG-CRN is an autoencoder network that takes point cloud inputs and predicts SDF primitives. The contrastive encoder network generates a feature vector for the difference in target geometry and the current reconstruction. Then the Multi-Level Perceptron (MLP) decoder network generates a SDF primitives to refine the reconstruction. The output parameters include a blending factor to better represent smooth, organic surfaces. The reconstruction can be fed back into the CSG-CRN network for an unlimited number of iterations to continue refining the reconstruction. Unlike previous work, the reconstruction time of CSG-CRN scales linearly with the level of detail. To our knowledge, this is the first approach that applies cascaded refinement to the synthesis of CSG models.

We make two main contributions in this work. First, we propose CSG-CRN, a novel architecture for generating CSG reconstructions through cascaded refinement. Second, we conduct experiments to demonstrate the following:

\begin{itemize}
	\item CSG-CRN can efficiently reconstruct a variety of known and unknown shape classes.
	\item CSG-CRN generates qualitatively and quantitatively superior reconstructions as compared to prior CSG-based work.
	\item CSG-CRN is capable of inferring missing geometry based on known shape priors.
	\item The CSG-CRN encoder produces a continuous latent space that can be interpolated.
\end{itemize}

\vspace{1em}

The outline of this paper is as follows. The next chapter, \nameref{chap:related_work}, reviews the 3D representations and neural architectures used in prior work. The \nameref{chap:background} chapter explains the mathematics of SDFs and the neural architectures used in our method. In the \nameref{chap:method} chapter, we provide a high-level view of the CSG-CRN architecture. The technical details of our implementation are provided in the \nameref{chap:implementation} chapter. The \nameref{chap:experiments_and_results} chapter contains the procedures and results for each experiment. Lastly, the \nameref{chap:conclusion} chapter summarizes our findings and discusses potential future work.
