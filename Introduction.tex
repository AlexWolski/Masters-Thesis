% !TeX root = Thesis.tex

\chapter{Introduction}
\label{chap:introduction}

Recent advances in science, technology, and engineering have created a demand for efficient 3D shape reconstruction and analysis. Applications include autonomous navigation, robotics, medicine, augmented reality, and entertainment~\cite{Xiao2020, Xie2022}. These fields require methods to quickly processes 3D scenes and accurately reconstruct surfaces from sparse sensor data.

Traditional methods for 3D shape analysis require features to be manually engineered using prior knowledge of a problem. Although engineered features are easy to interpret, the process is laborious and overlooks potentially useful information. Research efforts have since shifted to deep learning techniques that automatically learn the most relevant features~\cite{Bengio2013}. The improved feature extraction allows for more complex and varied reconstructions than is possible using traditional methods. In addition, the learned feature vectors can be applied to analysis problems such as object classification and inference of missing geometry~\cite{Park2019}.

A primary difficulty in applying deep learning to 3D reconstruction is selecting a suitable representation for the input and output of a network. A 3D representation must be concise enough for a network to effectively train on, yet must retain enough information about the original shape to enable an accurate reproduction. Related work employ a variety of 3D representations, each with unique benefits and challenges~\cite{Xiao2020}. We further discuss the different representations in the \nameref{sec:3d_representations} section of the \nameref{chap:related_work} chapter.

Recent work~\cite{Sharma2018, Kania2020, Ren2021} has explored the use of Constructive Solid Geometry (CSG) as a 3D representation. A CSG model defines a volume as a composition of simple shapes called primitives. Primitives are combined using boolean operations such as union, difference, and intersection. Each primitive is transformed using translation, rotation, and scaling operations. The selection of primitive shapes is restricted to a set of predefined manifold surfaces. Describing a shape using a fixed set of primitives and operations allows for a more compact format than other 3D representations. By virtue of their construction, all CSG models are continuous and manifold surfaces bounding an interior volume. Additionally, editing a CSG model is convenient since each constituent primitive can be modified individually~\cite{Hughes2013}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Union}
		\caption{Union}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Difference}
		\caption{Difference}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Intersect}
		\caption{Intersect}
	\end{subfigure}
	\caption{Three boolean operations applied to two spheres: (a) Union operation includes points encompassed by either sphere, (b) Difference operation includes points encompassed exclusively by the first sphere, (c) Intersect operation includes points encompassed by both the first and second spheres.}
	\label{fig:boolean operations}
\end{figure}

CSG and related primitive fitting techniques are topics of interest as they resemble Biederman's Recognition-By-Components theory~\cite{Biederman1987} for how humans comprehend 3D shapes. The theory suggests that the human brain deconstructs scenes into simple volumes called geons. Every brain is said to use the same set of 36 geon primitives to approximate complex shapes~\cite{Biederman1987}. Due to their similarity, researchers believe that the Recognition-By-Components theory sets a positive precedent for the use of CSG in deep learning~\cite{Sharma2018}.

Instead of using geons, prior work~\cite{Sharma2018, Kania2020, Ren2021} represents CSG primitives as Signed Distance Fields (SDFs). An SDF is a mathematical function that implicitly defines the surface of a volume. Given a query point, an SDF returns the shortest distance from the query point to the represented surface. This distance is positive ($+$) when the query point is outside of the volume and negative ($-$) when the query point is inside. We define the surface as the set of all input points for which the SDF returns 0 (i.e. the zero-level set). The implicit nature of SDFs results in smooth, detailed, and continuous surfaces~\cite{Park2019}. Additionally, SDFs make for good CSG primitives since applying boolean operations to distance fields is trivial.

\begin{figure}
	\centering
	\includegraphics[scale=0.2]{Images/SDF Box}
	\caption{Visualization of the SDF of a 2D box. The surface of the box is depicted in black, the exterior in green, and the interior in red. Points farther from the surface are colored darker. Each of the white isoline consist of points that are equidistant from the surface.}
	\label{fig:sdf_box}
\end{figure}

There are several benefits to using CSG models for deep 3D reconstruction. The low-dimensionality of a CSG model allows for more efficient training of the network. The continuous and watertight surfaces produced by CSG are visually pleasing. And a key benefit missing from most other representations is an easily modifiable format.

However, there are also several challenges in using a CSG representation. First is the lack of uniqueness. Two distinct sets of CSG operations may produce the same result. For instance, an L-shaped prism can be constructed by taking either the union or difference of two cuboids~\cite{Hughes2013}. This lack of uniqueness makes it difficult to supervise a model on a CSG dataset. Supervised training requires a ground-truth label to be provided for each input. Since there are multiple ways to construct a given object using CSG, there is no singular ground-truth label. Selecting only one of the multiple possible constructions as the ground-truth would restrict the solution space and inhibit training. As a result, sophisticated CSG reconstruction models are restricted to self-supervised or unsupervised learning. The second challenge is the tendency for sharp seams to form at the intersection between primitives. This makes modeling smooth and organic surfaces difficult. Seams can be reduced by blending primitives together, but the added feature comes at the cost of model complexity.

Prior work~\cite{Sharma2018, Kania2020, Ren2021} successfully implements CSG based reconstruction algorithms using unsupervised learning. While succeeding in representing simple geometry, these works fail to incorporate primitive blending to address the poor reconstruction quality of smooth surfaces. Furthermore, these models generate a small number of primitives. The CSG-Stump architecture~\cite{Ren2021} supports a maximum output of 256 total primitives in the paper, of which a subset is selected for use in reconstruction. This number is sufficient to represent simple 3D shapes but fails to capture the details in more complex geometry. Although the architecture can be scaled up, the network complexity, memory requirements, and training time increase non-linearly with the output size~\cite{Ren2021}.

\begin{figure}[!b]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Face without blending}
		\caption{Without Blending}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/Face with blending}
		\caption{With Blending}
	\end{subfigure}
	\caption{Demonstration of the importance of primitive blending for modeling organic shapes such as faces using CSG~\cite{Quilez2013}.}
	\label{fig:primitive_blending}
\end{figure}

To address these shortcomings, we propose the Constructive Solid Geometry Cascaded Refinement Network (CSG-CRN): a novel architecture for iteratively generating CSG reconstructions of unlimited size. This is achieved using cascaded refinement, wherein a refinement network recursively improves upon its own output. By iteratively building a reconstruction, we can achieve high reconstruction quality using a small architecture. CSG-CRN is an autoencoder network that takes point cloud inputs and predicts one SDF primitive. The Siamese encoder network generates a feature vector for both the target geometry and the current reconstruction. Then the MLP decoder network analyzes the differences in the feature vectors and generates a single SDF primitive to refine the reconstruction. The output parameters include a blending factor to better represent smooth, organic surfaces. The reconstruction can be fed back into the CSG-CRN network for an unlimited number of iterations to continue refining the reconstruction. Unlike previous work, the reconstruction time of CSG-CRN scales linearly with the level of detail. To our knowledge, this is the first approach that applies cascaded refinement to the synthesis of CSG models.

We make two main contributions in this work. First, we propose CSG-CRN, a novel architecture for generating CSG reconstructions through cascaded refinement. Second, we conduct experiments to demonstrate the following:

\begin{itemize}
	\item CSG-CRN can efficiently reconstruct a variety of known and unknown shape classes.
	\item CSG-CRN generates qualitatively and quantitatively superior reconstructions as compared to prior CSG-based work.
	\item CSG-CRN is capable of inferring missing geometry based on known shape priors.
	\item The CSG-CRN encoder produces a continuous latent space that can be interpolated.
\end{itemize}

\vspace{1em}

The outline of this paper is as follows. The next chapter, \nameref{chap:related_work}, reviews the 3D representations and neural architectures used in prior work. The \nameref{chap:background} chapter explains the mathematics of SDFs and the neural architectures used in our method. In the \nameref{chap:method} chapter, we provide a high-level view of the CSG-CRN architecture. The technical details of our implementation are provided in the \nameref{chap:implementation} chapter. The \nameref{chap:experiments_and_results} chapter contains the procedures and results for each experiment. Lastly, the \nameref{chap:conclusion} chapter summarizes our findings and discusses potential future work.
