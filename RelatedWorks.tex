% !TeX root = Thesis.tex

\chapter{Related~Works}
\label{chap:related_works}

In this chapter, we review prior works on two topics related to our research: \nameref{sec:3d_representations}s and \nameref{sec:representation_learning_architectures}.

\section{3D~Representations}
\label{sec:3d_representations}

There are a multitude of methods for digitally representing a 3D volume. Any representation can be used in machine learning, but each has different benefits and challenges. In this section, we discuss the five most common 3D representations in the literature and their applications in machine learning.


\subsection{Point~Cloud}
\label{subsec:point_cloud}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{Images/Point Cloud Cup}
	\caption{Point cloud representation of a coffee cup.}
	\label{fig:point_cloud_cup}
\end{figure}

A point cloud is set of unordered 3D coordinates sampled from an object's surface. Point clouds are unstructured and lack topological data~\cite{Xiao2020}. However, this simplicity makes it easy to acquire point clouds using 3D scanning technologies such as lidar and photogrammetry~\cite{Leberl2010}. Photogrammetry is an algorithm that uses multiple photos taken from different angles to triangulate points on a surface. A photogrammetry scan can be performed with even a smartphone, greatly increasing the accessibility of point cloud scans~\cite{Micheletti2015}.

Despite its accessibility, point clouds didn't see use in 3D learning techniques until 2017~\cite{Xiao2020}. The seminal work PointNet~\cite{Qi2017} used a Multi-Level Perceptron (MLP) encoder to demonstrate that point clouds can be processed directly to solve problems such as classification and part segmentation. The follow up work PointConv~\cite{Wu2019} saw similar success using a Convolutional Neural Network (CNN) encoder. Further works~\cite{Fan2017, Achlioptas2018} have demonstrated that deep learning models can be trained to reconstruct an input as a point cloud. Although point clouds only offer a sparse approximation of a surface, their low-dimensionality proves useful in efficiently learning 3D features.


\subsection{Polygon~Mesh}
\label{subsec:polygon_mesh}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{Images/Mesh Cup}
	\caption{Triangle mesh representation of a coffee cup. The black lines depict the connecting edges between vertices.}
	\label{fig:mesh_cup}
\end{figure}

Mesh-based models consist of vertices connected by flat polygonal faces. Unlike point clouds, meshes can represent topology and surface details~\cite{Xiao2020}. Polygon meshes are the most popular 3D representation for use in computer graphics because they can be efficiently rendered as images~\cite{Watt1996}. Another unique benefit is that meshes are easy to deform and animate~\cite{Wang2018}. These qualities make mesh-based models ideal for use in video games, virtual reality, and film~\cite{Nash2020}. One issue is that mesh models cannot be directly acquired using 3D scanners. This is easily circumvented, however, by using surface reconstruction techniques. Algorithms such as marching cubes can reconstruct point cloud data as high-quality mesh models~\cite{Yuan2022}.

The prevalence of polygon meshes has motivated many mesh-based deep learning models. Since the structure of a polygon mesh is complex, these models adopt unique architectures to simplify the problem. To directly process polygon meshes, models such as FeaStNet~\cite{Verma2018} and MeshCNN~\cite{Hanocka2019} employ Graph Convolutional Networks (GCN). A mesh can be viewed as a directed graph of interconnected nodes. This enables graph-based methods such as GCN to be applied to meshes. While a traditional CNN applies filters to a group of pixels in an image, a GCN applies filters to a group of nodes or edges in a graph. A GCN can incorporate pooling operations to efficiently extract and analyze features from a mesh~\cite{Verma2018}. MeshCNN also introduces unpooling operations to increase the resolution of a mesh.

Techniques to generate meshes are more varied. AtlasNet~\cite{Groueix2018} uses MLPs to generate a series of 2D meshes and stitch them together into a 3D shape. This papier-m\^{a}ch\'{e} approach is efficient but does not generate a continuous surface~\cite{Groueix2018}. Pixel2Mesh~\cite{Wang2018} and Point2Mesh~\cite{Hanocka2020} both deform a template mesh through cascaded refinement to represent the input. Deformations are performed in a coarse-to-fine manner with the resolution of the template mesh increasing each iteration. This approach is efficient because the low-resolution template is used for large deformations while the high-resolution template is only used for precise deformations. Unlike AtlasNet, Pixel2Mesh and Point2Mesh are guaranteed to generate a continuous and manifold surfaces since the initial template shape is manifold~\cite{Wang2018, Hanocka2020}. PolyGen~\cite{Nash2020} takes the most direct approach in generating 3D meshs. A vertex generation network predicts a variable number of vertices to represent the input. Then the vertices are fed to a face generation network that predicts how to best them with faces. Both networks are implemented as sequential transformers. Polygen can generate variable size meshes but has a fixed maximum output resolution~\cite{Nash2020}.


\subsection{Occupancy~Grid}
\label{subsec:occupancy_grid}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{Images/Voxel Cup}
	\caption{Voxel grid representation of a coffee cup.}
	\label{fig:voxel_cup}
\end{figure}

\subsection{Images}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{Images/Depth Map Cup}
	\caption{Depth map representation of a coffee cup.}
	\label{fig:depth_map_cup}
\end{figure}

\subsection{Implicit~Surface}
\label{subsec:implicit_surface}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{Images/SDF Cup}
	\caption{Visualization of an SDF representing a coffee cup. Points close to the surface are drawn green and points far from the surface are drawn blue. Each white contour line consists of points that are equidistant from the surface.}
	\label{fig:sdf_cup}
\end{figure}

\subsection{Structured~Representation}
\label{subsec:structured_representation}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{Images/CSG Cup}
	\caption{The CSG tree of a coffee cup. The leaves of the tree are SDF primitives which are combined through union (+) and difference (-) operations. The primitives being subtracted are colored red.}
	\label{fig:csg_cup}
\end{figure}


\section{Representation~Learning~Architectures}
\label{sec:representation_learning_architectures}

Test

\subsection{Generative~Adversarial~Network}
\label{subsec:generative_adversarial_networks}

Test

\subsection{Autoencoder}
\label{subsec:autoencoders}

Test

\subsection{Cascaded~Refinement}
\label{subsec:cascaded_refinement}

Test